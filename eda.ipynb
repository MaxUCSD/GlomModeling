{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08746728",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5883e129",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e80a414",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = 'Stimulus_Screen_Data/variables_for_fitting_activity/'\n",
    "HZ = 10\n",
    "\n",
    "\n",
    "def analyze_neural_activity_around_tap(tap_events, neural_data, tap_index=0, \n",
    "                                     time_before=10, time_after=10, hz=1000,\n",
    "                                     neuron_selection='auto', neuron_indices=None):\n",
    "    \"\"\"\n",
    "    Analyze neural activity around a specific tap event.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    tap_events : array-like\n",
    "        Array of tap event timestamps (in samples)\n",
    "    neural_data : numpy.ndarray\n",
    "        Neural data matrix (neurons x time)\n",
    "    tap_index : int, default=0\n",
    "        Index of the tap event to analyze\n",
    "    time_before : float, default=10\n",
    "        Time window before tap (in seconds)\n",
    "    time_after : float, default=10\n",
    "        Time window after tap (in seconds)\n",
    "    hz : int, default=1000\n",
    "        Sampling rate (Hz)\n",
    "    neuron_selection : str, default='auto'\n",
    "        'auto' to pick neurons with highest variance, 'manual' to use neuron_indices\n",
    "    neuron_indices : tuple, optional\n",
    "        Tuple of (neuron1_idx, neuron2_idx) for manual selection\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary containing analysis results and plot data\n",
    "    \"\"\"\n",
    "    \n",
    "    # Input validation\n",
    "    if tap_index >= len(tap_events):\n",
    "        raise ValueError(f\"tap_index {tap_index} is out of range for tap_events length {len(tap_events)}\")\n",
    "    \n",
    "    # Get the tap time\n",
    "    tap_time = tap_events[tap_index]\n",
    "    \n",
    "    # Convert time windows to samples\n",
    "    time_before_samples = int(time_before * hz)\n",
    "    time_after_samples = int(time_after * hz)\n",
    "    \n",
    "    # Check if we have enough data around this tap\n",
    "    if tap_time < time_before_samples or tap_time + time_after_samples >= neural_data.shape[1]:\n",
    "        raise ValueError(f\"Not enough data around tap at sample {tap_time}\")\n",
    "    \n",
    "    # Extract the time window around this tap\n",
    "    start_idx = tap_time - time_before_samples\n",
    "    end_idx = tap_time + time_after_samples\n",
    "    neural_window = neural_data[:, start_idx:end_idx]\n",
    "    \n",
    "    # Select neurons to analyze\n",
    "    if neuron_selection == 'auto':\n",
    "        neuron_responses = np.var(neural_window, axis=1)\n",
    "        neuron1_idx, neuron2_idx = np.argsort(neuron_responses)[::-1][:2]\n",
    "    elif neuron_selection == 'manual':\n",
    "        if neuron_indices is None:\n",
    "            raise ValueError(\"neuron_indices must be provided when neuron_selection='manual'\")\n",
    "        neuron1_idx, neuron2_idx = neuron_indices\n",
    "    else:\n",
    "        raise ValueError(\"neuron_selection must be 'auto' or 'manual'\")\n",
    "    \n",
    "    # Extract the two neurons' activity\n",
    "    neuron1_activity = neural_window[neuron1_idx, :]\n",
    "    neuron2_activity = neural_window[neuron2_idx, :]\n",
    "    \n",
    "    # Create time points in absolute time (seconds)\n",
    "    absolute_time_points = (np.arange(len(neuron1_activity)) + start_idx) / hz\n",
    "    \n",
    "    # Calculate tap positions and times within the window\n",
    "    tap_list = []\n",
    "    tap_times_absolute = []\n",
    "    tap_indices_absolute = []\n",
    "    \n",
    "    for i, tap in enumerate(tap_events):\n",
    "        relative_position = (tap - tap_time) + time_before_samples\n",
    "        if 0 <= relative_position < neural_window.shape[1]:\n",
    "            tap_list.append(relative_position)\n",
    "            tap_times_absolute.append(tap / hz)\n",
    "            tap_indices_absolute.append(i)\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Top left: Neuron 1 vs time\n",
    "    axes[0,0].plot(absolute_time_points, neuron1_activity, 'b-', linewidth=2)\n",
    "    for local_idx, (tap, tap_time_abs, abs_idx) in enumerate(zip(tap_list, tap_times_absolute, tap_indices_absolute)):\n",
    "        tap_time_on_plot = absolute_time_points[tap]\n",
    "        axes[0,0].axvline(tap_time_on_plot, color='r', linestyle='--', linewidth=2, \n",
    "                         label=f'Tap #{abs_idx} (t={tap_time_abs:.2f}s)')\n",
    "    \n",
    "    axes[0,0].set_xlabel('Time (seconds)')\n",
    "    axes[0,0].set_ylabel('Firing Rate')\n",
    "    axes[0,0].set_title(f'Neuron {neuron1_idx} vs Time')\n",
    "    axes[0,0].grid(True, alpha=0.3)\n",
    "    axes[0,0].legend()\n",
    "    \n",
    "    # Top right: Neuron 2 vs time\n",
    "    axes[0,1].plot(absolute_time_points, neuron2_activity, 'g-', linewidth=2)\n",
    "    for local_idx, (tap, tap_time_abs, abs_idx) in enumerate(zip(tap_list, tap_times_absolute, tap_indices_absolute)):\n",
    "        tap_time_on_plot = absolute_time_points[tap]\n",
    "        axes[0,1].axvline(tap_time_on_plot, color='r', linestyle='--', linewidth=2, \n",
    "                         label=f'Tap #{abs_idx} (t={tap_time_abs:.2f}s)')\n",
    "    \n",
    "    axes[0,1].set_xlabel('Time (seconds)')\n",
    "    axes[0,1].set_ylabel('Firing Rate')\n",
    "    axes[0,1].set_title(f'Neuron {neuron2_idx} vs Time')\n",
    "    axes[0,1].grid(True, alpha=0.3)\n",
    "    axes[0,1].legend()\n",
    "    \n",
    "    # Bottom: Trajectory plot (spanning both bottom subplots)\n",
    "    axes[1,0].remove()\n",
    "    axes[1,1].remove()\n",
    "    ax_traj = fig.add_subplot(2, 1, 2)\n",
    "    \n",
    "    # Plot the trajectory with time-based coloring\n",
    "    scatter = ax_traj.scatter(neuron1_activity, neuron2_activity, c=absolute_time_points, \n",
    "                             cmap='viridis', s=30, alpha=0.7)\n",
    "    \n",
    "    # Mark the tap times\n",
    "    for local_idx, (tap, tap_time_abs, abs_idx) in enumerate(zip(tap_list, tap_times_absolute, tap_indices_absolute)):\n",
    "        ax_traj.plot(neuron1_activity[tap], neuron2_activity[tap], 'ro', \n",
    "                    markersize=12, label=f'Tap #{abs_idx} (t={tap_time_abs:.2f}s)', \n",
    "                    markeredgecolor='white', markeredgewidth=2)\n",
    "    \n",
    "    # Add colorbar\n",
    "    cbar = plt.colorbar(scatter, ax=ax_traj)\n",
    "    cbar.set_label('Time (seconds)')\n",
    "    \n",
    "    ax_traj.set_xlabel(f'Neuron {neuron1_idx} Firing Rate')\n",
    "    ax_traj.set_ylabel(f'Neuron {neuron2_idx} Firing Rate')\n",
    "    ax_traj.set_title('Neural State Trajectory (Colored by Time)')\n",
    "    ax_traj.legend()\n",
    "    ax_traj.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary information\n",
    "    print(f\"Analyzing tap #{tap_index} at sample {tap_time}\")\n",
    "    print(f\"Selected neurons: {neuron1_idx} and {neuron2_idx}\")\n",
    "    print(f\"Taps in window at samples: {tap_list}\")\n",
    "    print(f\"Tap times in seconds: {[f'{t:.2f}s' for t in tap_times_absolute]}\")\n",
    "    print(f\"Absolute tap indices: {tap_indices_absolute}\")\n",
    "    \n",
    "    # Return results dictionary\n",
    "    return {\n",
    "        'neuron1_idx': neuron1_idx,\n",
    "        'neuron2_idx': neuron2_idx,\n",
    "        'neuron1_activity': neuron1_activity,\n",
    "        'neuron2_activity': neuron2_activity,\n",
    "        'absolute_time_points': absolute_time_points,\n",
    "        'tap_list': tap_list,\n",
    "        'tap_times_absolute': tap_times_absolute,\n",
    "        'tap_indices_absolute': tap_indices_absolute,\n",
    "        'neural_window': neural_window,\n",
    "        'start_idx': start_idx,\n",
    "        'end_idx': end_idx\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "def clean_neural_data(all_neural_data, good_exps, zero_threshold=0.5, verbose=True, return_structure='nested'):\n",
    "    \"\"\"\n",
    "    Clean neural data by removing neurons with high zero proportions.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    all_neural_data : dict\n",
    "        Nested dictionary: all_neural_data[animal][stimuli] = 2D array\n",
    "        where rows are neurons and columns are time points/trials\n",
    "    good_exps : dict\n",
    "        Dictionary mapping animals to lists of stimuli\n",
    "    zero_threshold : float, default=0.5\n",
    "        Threshold for proportion of zeros (neurons above this are removed)\n",
    "    verbose : bool, default=True\n",
    "        Whether to print progress information\n",
    "    return_structure : str, default='nested'\n",
    "        'nested' returns cleaned_data[animal][stimuli] structure\n",
    "        'dict' returns {'cleaned_data': ..., 'bad_neuron_indices': ...}\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        If return_structure='nested': nested dict with cleaned_data[animal][stimuli]\n",
    "        If return_structure='dict': dict with 'cleaned_data' and 'bad_neuron_indices' keys\n",
    "    \"\"\"\n",
    "    \n",
    "    def get_high_zero_neurons(data, threshold):\n",
    "        \"\"\"Helper function to find neurons with high zero proportions.\"\"\"\n",
    "        zero_proportions = np.mean(data == 0, axis=1)\n",
    "        return np.where(zero_proportions > threshold)[0]\n",
    "    \n",
    "    # Find bad neurons for each animal (across all stimuli)\n",
    "    all_bad_neurons = {}\n",
    "    \n",
    "    for animal in good_exps:\n",
    "        animal_bad_inds = []\n",
    "        \n",
    "        for stimuli in good_exps[animal]:\n",
    "            data = all_neural_data[animal][stimuli]\n",
    "            bad_inds = get_high_zero_neurons(data, zero_threshold)\n",
    "            animal_bad_inds.extend(bad_inds)\n",
    "        \n",
    "        # Remove duplicates\n",
    "        animal_bad_inds = np.unique(animal_bad_inds)\n",
    "        all_bad_neurons[animal] = animal_bad_inds\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Animal {animal}: {len(animal_bad_inds)} neurons with >{zero_threshold*100}% zeros\")\n",
    "    \n",
    "    # Clean the data by removing bad neurons\n",
    "    all_exps_cleaned = {}\n",
    "    \n",
    "    for animal in good_exps:\n",
    "        bad_indices = all_bad_neurons[animal]\n",
    "        all_exps_cleaned[animal] = {}  # Create nested structure\n",
    "        \n",
    "        for stimuli in good_exps[animal]:\n",
    "            data = all_neural_data[animal][stimuli]\n",
    "            \n",
    "            # Keep only good neurons\n",
    "            good_indices = np.setdiff1d(np.arange(data.shape[0]), bad_indices)\n",
    "            cleaned_data = data[good_indices, :]\n",
    "            \n",
    "            all_exps_cleaned[animal][stimuli] = cleaned_data\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"Animal {animal}, Stimuli {stimuli}: \"\n",
    "                      f\"{data.shape[0]} -> {cleaned_data.shape[0]} neurons \"\n",
    "                      f\"(removed {len(bad_indices)})\")\n",
    "    \n",
    "    # Return based on requested structure\n",
    "    if return_structure == 'nested':\n",
    "        return all_exps_cleaned\n",
    "    else:\n",
    "        return {\n",
    "            'cleaned_data': all_exps_cleaned,\n",
    "            'bad_neuron_indices': all_bad_neurons\n",
    "        }\n",
    "    \"\"\"\n",
    "    Clean neural data by removing neurons with high zero proportions.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    all_neural_data : dict\n",
    "        Nested dictionary: all_neural_data[animal][stimuli] = 2D array\n",
    "        where rows are neurons and columns are time points/trials\n",
    "    good_exps : dict\n",
    "        Dictionary mapping animals to lists of stimuli\n",
    "    zero_threshold : float, default=0.5\n",
    "        Threshold for proportion of zeros (neurons above this are removed)\n",
    "    verbose : bool, default=True\n",
    "        Whether to print progress information\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary with keys:\n",
    "        - 'cleaned_data': dict mapping (animal, stimuli) tuples to cleaned arrays\n",
    "        - 'bad_neuron_indices': dict mapping animals to arrays of bad neuron indices\n",
    "    \"\"\"\n",
    "    \n",
    "    def get_high_zero_neurons(data, threshold):\n",
    "        \"\"\"Helper function to find neurons with high zero proportions.\"\"\"\n",
    "        zero_proportions = np.mean(data == 0, axis=1)\n",
    "        return np.where(zero_proportions > threshold)[0]\n",
    "    \n",
    "    # Find bad neurons for each animal (across all stimuli)\n",
    "    all_bad_neurons = {}\n",
    "    \n",
    "    for animal in good_exps:\n",
    "        animal_bad_inds = []\n",
    "        \n",
    "        for stimuli in good_exps[animal]:\n",
    "            data = all_neural_data[animal][stimuli]\n",
    "            bad_inds = get_high_zero_neurons(data, zero_threshold)\n",
    "            animal_bad_inds.extend(bad_inds)\n",
    "        \n",
    "        # Remove duplicates\n",
    "        animal_bad_inds = np.unique(animal_bad_inds)\n",
    "        all_bad_neurons[animal] = animal_bad_inds\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Animal {animal}: {len(animal_bad_inds)} neurons with >{zero_threshold*100}% zeros\")\n",
    "    \n",
    "    # Clean the data by removing bad neurons\n",
    "    all_exps_cleaned = {}\n",
    "    \n",
    "    for animal in good_exps:\n",
    "        bad_indices = all_bad_neurons[animal]\n",
    "        \n",
    "        for stimuli in good_exps[animal]:\n",
    "            data = all_neural_data[animal][stimuli]\n",
    "            \n",
    "            # Keep only good neurons\n",
    "            good_indices = np.setdiff1d(np.arange(data.shape[0]), bad_indices)\n",
    "            cleaned_data = data[good_indices, :]\n",
    "            \n",
    "            all_exps_cleaned[(animal, stimuli)] = cleaned_data\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"Animal {animal}, Stimuli {stimuli}: \"\n",
    "                      f\"{data.shape[0]} -> {cleaned_data.shape[0]} neurons \"\n",
    "                      f\"(removed {len(bad_indices)})\")\n",
    "    \n",
    "    return {\n",
    "        'cleaned_data': all_exps_cleaned,\n",
    "        'bad_neuron_indices': all_bad_neurons\n",
    "    }\n",
    "\n",
    "\n",
    "def load_pickle(fn):\n",
    "    full_fp = f'{folder}{fn}'\n",
    "    with open(full_fp, 'rb') as file:\n",
    "        data = pickle.load(file)\n",
    "    return data\n",
    "\n",
    "\n",
    "def plot_heat_touch(neural_data, touches, title='Response to touch', vmax=None, ax=None):\n",
    "    \"\"\"\n",
    "    Plot neural data heatmap with touch indicators.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    neural_data : array-like\n",
    "        2D array of neural data\n",
    "    touches : array-like\n",
    "        Array of touch timepoints\n",
    "    title : str\n",
    "        Plot title\n",
    "    vmax : float, optional\n",
    "        Maximum value for colormap\n",
    "    ax : matplotlib.axes.Axes, optional\n",
    "        Axes to plot on. If None, creates new figure\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple : (ax, im)\n",
    "        The axes object and image object for colorbar creation\n",
    "    \"\"\"\n",
    "    # If no axes provided, create a new figure\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        show_plot = True\n",
    "    else:\n",
    "        show_plot = False\n",
    "    \n",
    "    # Set up imshow parameters\n",
    "    imshow_kwargs = {'aspect': 'auto', 'origin': 'lower', \n",
    "                     'extent': [0, neural_data.shape[1]/HZ, 0, neural_data.shape[0]]}\n",
    "    if vmax is not None:\n",
    "        imshow_kwargs['vmax'] = vmax\n",
    "    \n",
    "    # Create the heatmap\n",
    "    im = ax.imshow(neural_data, **imshow_kwargs)\n",
    "    \n",
    "    # Set limits and labels\n",
    "    ax.set_ylim(0, neural_data.shape[0])\n",
    "    ax.set_ylabel('neuron index')\n",
    "    ax.set_xlim((0, neural_data.shape[1]/HZ))\n",
    "    ax.set_xlabel('time (s)')\n",
    "    ax.set_title(title)\n",
    "    \n",
    "    # Add touch indicators\n",
    "    if not touches:\n",
    "        print('Warning: touch is empty')\n",
    "    else: \n",
    "        for i, touch in enumerate(touches):\n",
    "            if i == 0:\n",
    "                ax.axvline(touch/HZ, label='touch', color='red', alpha=0.7)\n",
    "            else:\n",
    "                ax.axvline(touch/HZ, color='red', alpha=0.7)\n",
    "        \n",
    "        ax.legend()\n",
    "    \n",
    "    # Only show if this is a standalone plot\n",
    "    if show_plot:\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    return ax, im  # Return both axes and image object\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934dd4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "animals = ['2792', '3012', '3045', 'F1', 'F2', 'F3', '7132'] #Which are male and which are female?\n",
    "stimulus = ['Fresh FMU', 'MMU 2', 'feces', 'blood', 'bobcat', 'owl pellet', 'Snake', 'FMU-1', 'FMU-2', 'old FMU'] #Okay so multiple experiments are handles by appending a number, either like -1 or 2?\n",
    "# animal: 3045\n",
    "# stimuli: bobcat  Data look really weird. Jen said it was fine, but gonna remove for now.\n",
    "good_exps = {'3012': ['Fresh FMU', 'MMU 2', 'feces','blood', 'bobcat','Snake' ],\n",
    "                         '3045':  ['Fresh FMU', 'MMU 2', 'feces', 'blood', 'Snake'],\n",
    "                         'F1': ['Fresh FMU', 'feces', 'blood', 'bobcat',  'Snake'],\n",
    "                         'F2': ['Fresh FMU', 'MMU 2','blood','bobcat', 'Snake'],\n",
    "                         'F3': ['Fresh FMU', 'MMU 2','feces','blood', 'bobcat']}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee4d740",
   "metadata": {},
   "source": [
    "# Key Info\n",
    "\n",
    "C_norm: neural activity {animal:{stimulus:[glomerulixframes]}} 10 frames/s (Q: Is there multiple experiments per animal?)\n",
    "\n",
    "Stimulus_added : frame when stimulus added ()\n",
    "\n",
    "Stimulus_delivered: frame when stimulus applied to nose of mouse\n",
    "\n",
    "Stimulus_removed: frame when stimulus removed from arena\n",
    "\n",
    "Touch_first_mini_frame: first frame of every tapping bout (in miniscope frames)\n",
    "\n",
    "Touch_first: first time of every tapping bout in 500 Hz recording from touch sensor (I'm gonna ignore this?)\n",
    "\n",
    "Touch_mini_frame: miniscope frame of every nose tap of that stimulus (This is what we care about)\n",
    "\n",
    "Touch_per_bout: number of nose taps per bout (How is a bout defined relative to 'Touch_mini_frame'?)\n",
    "\n",
    "Touch: binary array at 500Hz of touch sensor data. 1= nose tap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6fa77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def our_std(x, mu):\n",
    "#     return np.sqrt(np.sum((x - mu)**2)/max(1, (len(x)-1)))\n",
    "\n",
    "# def our_normalization(neural_data, Verbose=1):\n",
    "#     \"Input (neural_data): neural_data.shape = (N x T), N is neurons and T is time steps\"\n",
    "#     \"Returns (normalization_values): normalization_values.shape =(N x 2) N is neurons and [mu, std]\"\n",
    "#     neural_data_original = neural_data\n",
    "#     neural_data = neural_data[:, :2000]\n",
    "#     pop_sum = np.sum(neural_data, axis=0)\n",
    "#     pop_med = np.median(pop_sum) \n",
    "#     pop_sum_le_med = pop_sum[pop_sum < pop_med] #grab values below the median\n",
    "#     # Compute the std over the values below the median using the median as the mean.\n",
    "#     pop_le_med_std = our_std(pop_sum_le_med, pop_med) #pop_le_med_std = np.sqrt(np.sum((pop_sum_le_med - pop_med)**2)/len(pop_sum_le_med)-1)\n",
    "#     mask = pop_sum < (pop_med + (2 * pop_le_med_std))\n",
    "#     if Verbose==1:\n",
    "#         plt.hist(pop_sum, bins=50)\n",
    "#         plt.axvline(pop_med, c='r', label=f'median = {round(pop_med)}')\n",
    "#         plt.axvline(pop_med + (2 * pop_le_med_std), c='g', label=f'2$\\sigma$ (our method) + median = {round(pop_med + (2 * pop_le_med_std))}')\n",
    "\n",
    "#         plt.title('Population Sum Histogram')\n",
    "#         plt.ylabel('Counts')\n",
    "#         plt.xlabel('Activity Level ')\n",
    "#         plt.legend()\n",
    "#         plt.show()\n",
    "\n",
    "\n",
    "#     # individual level now\n",
    "#     normalization_values = np.zeros((neural_data.shape[0], 2))\n",
    "    \n",
    "#     for i in range(neural_data.shape[0]):\n",
    "#         individual_neural_data_pop_masked = neural_data[i, mask]\n",
    "#         individual_mu = np.median(individual_neural_data_pop_masked)\n",
    "#         individual_non_zero_neural_data = individual_neural_data_pop_masked[individual_neural_data_pop_masked != 0]\n",
    "#         #temp = individual_non_zero_neural_data[individual_non_zero_neural_data < individual_mu]\n",
    "#         individual_std = our_std(individual_non_zero_neural_data, individual_mu)\n",
    "#         normalization_values[i,0] = individual_mu\n",
    "#         normalization_values[i,1] = individual_std\n",
    "#         if Verbose == 2:\n",
    "#             if i % 50 == 0:\n",
    "                \n",
    "#                 plt.hist(individual_non_zero_neural_data, bins=50)\n",
    "#                 plt.axvline(individual_mu, c='r', label=f'median = {(individual_mu)}')\n",
    "\n",
    "#                 plt.title(f'Individual Histogram Neuron #{i}')\n",
    "#                 plt.ylabel('Counts')\n",
    "#                 plt.xlabel('Activity Level ')\n",
    "#                 plt.legend()\n",
    "#                 plt.show()\n",
    "        \n",
    "#     normalized_data = (neural_data_original - normalization_values[:, [0]]) / (normalization_values[:, [1]] + 1e-4)\n",
    "\n",
    "\n",
    "#     return normalization_values, normalized_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8139bc77",
   "metadata": {},
   "source": [
    "# Some Animals have days where a specific neuron is like almost always = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7207c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def our_std(x, mu):\n",
    "    return np.sqrt(np.sum((x - mu)**2)/max(1, (len(x)-1)))\n",
    "\n",
    "\n",
    "def our_normalization(neural_data, Verbose=1, min_std_threshold='auto', fallback_method='median_mad'):\n",
    "    \"\"\"\n",
    "    Input (neural_data): neural_data.shape = (N x T), N is neurons and T is time steps\n",
    "    Returns (normalization_values): normalization_values.shape =(N x 2) N is neurons and [mu, std]\n",
    "    \n",
    "    Parameters:\n",
    "    - min_std_threshold: minimum allowed standard deviation to prevent division by tiny numbers\n",
    "      - float: fixed threshold value\n",
    "      - 'auto': automatically determine based on data distribution\n",
    "      - 'percentile_X': use Xth percentile of all computed stds (e.g., 'percentile_5')\n",
    "      - 'mad_based': use population MAD scaled as threshold\n",
    "    - fallback_method: what to do when std is too small\n",
    "      - 'median_mad': use median absolute deviation scaled to match std\n",
    "      - 'global_std': use a global std estimate across all neurons\n",
    "      - 'fixed_std': use a fixed minimum std value\n",
    "    \"\"\"\n",
    "    neural_data_original = neural_data\n",
    "    neural_data = neural_data[:, :2000]\n",
    "    pop_sum = np.sum(neural_data, axis=0)\n",
    "    pop_med = np.median(pop_sum) \n",
    "    pop_sum_le_med = pop_sum[pop_sum < pop_med]\n",
    "    pop_le_med_std = our_std(pop_sum_le_med, pop_med)\n",
    "    mask = pop_sum < (pop_med + (2 * pop_le_med_std))\n",
    "    \n",
    "    if Verbose==1:\n",
    "        plt.hist(pop_sum, bins=50)\n",
    "        plt.axvline(pop_med, c='r', label=f'median = {round(pop_med)}')\n",
    "        plt.axvline(pop_med + (2 * pop_le_med_std), c='g', label=f'2$\\sigma$ (our method) + median = {round(pop_med + (2 * pop_le_med_std))}')\n",
    "        plt.title('Population Sum Histogram')\n",
    "        plt.ylabel('Counts')\n",
    "        plt.xlabel('Activity Level ')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    # First pass: compute all individual stds to determine dynamic threshold\n",
    "    all_individual_stds = []\n",
    "    temp_normalization_data = []\n",
    "    \n",
    "    for i in range(neural_data.shape[0]):\n",
    "        individual_neural_data_pop_masked = neural_data[i, mask]\n",
    "        individual_mu = np.median(individual_neural_data_pop_masked)\n",
    "        individual_non_zero_neural_data = individual_neural_data_pop_masked[individual_neural_data_pop_masked != 0]\n",
    "        individual_std = our_std(individual_non_zero_neural_data, individual_mu)\n",
    "        \n",
    "        all_individual_stds.append(individual_std)\n",
    "        temp_normalization_data.append((individual_mu, individual_std, individual_non_zero_neural_data))\n",
    "    \n",
    "    all_individual_stds = np.array(all_individual_stds)\n",
    "    \n",
    "    # Determine dynamic threshold\n",
    "    if isinstance(min_std_threshold, str):\n",
    "        if min_std_threshold == 'auto':\n",
    "            # Use a robust method: median - 2*MAD of the std distribution\n",
    "            std_median = np.median(all_individual_stds)\n",
    "            std_mad = np.median(np.abs(all_individual_stds - std_median))\n",
    "            min_std_threshold = max(std_median - 2*std_mad, std_median * 0.1)  # Don't go below 10% of median\n",
    "            if Verbose >= 1:\n",
    "                print(f'Auto-determined min_std_threshold: {min_std_threshold:.6f}')\n",
    "                print(f'  (std_median: {std_median:.6f}, std_mad: {std_mad:.6f})')\n",
    "        \n",
    "        elif min_std_threshold.startswith('percentile_'):\n",
    "            percentile = float(min_std_threshold.split('_')[1])\n",
    "            min_std_threshold = np.percentile(all_individual_stds, percentile)\n",
    "            if Verbose >= 1:\n",
    "                print(f'Using {percentile}th percentile as min_std_threshold: {min_std_threshold:.6f}')\n",
    "        \n",
    "        elif min_std_threshold == 'mad_based':\n",
    "            # Use the population-level MAD as a reference\n",
    "            all_data_flat = neural_data[mask].flatten()\n",
    "            all_data_nonzero = all_data_flat[all_data_flat != 0]\n",
    "            pop_median = np.median(all_data_nonzero)\n",
    "            pop_mad = np.median(np.abs(all_data_nonzero - pop_median))\n",
    "            min_std_threshold = pop_mad * 1.4826 * 0.1  # 10% of population std estimate\n",
    "            if Verbose >= 1:\n",
    "                print(f'MAD-based min_std_threshold: {min_std_threshold:.6f}')\n",
    "                print(f'  (pop_mad: {pop_mad:.6f}, scaled: {pop_mad * 1.4826:.6f})')\n",
    "        \n",
    "        elif min_std_threshold == 'outlier_detection':\n",
    "            # Use statistical outlier detection on the std distribution\n",
    "            Q1 = np.percentile(all_individual_stds, 25)\n",
    "            Q3 = np.percentile(all_individual_stds, 75)\n",
    "            IQR = Q3 - Q1\n",
    "            min_std_threshold = max(Q1 - 1.5 * IQR, np.percentile(all_individual_stds, 1))\n",
    "            if Verbose >= 1:\n",
    "                print(f'Outlier-detection min_std_threshold: {min_std_threshold:.6f}')\n",
    "                print(f'  (Q1: {Q1:.6f}, Q3: {Q3:.6f}, IQR: {IQR:.6f})')\n",
    "        \n",
    "        else:\n",
    "            # Default fallback\n",
    "            min_std_threshold = 0.01\n",
    "            if Verbose >= 1:\n",
    "                print(f'Unknown threshold method, using default: {min_std_threshold}')\n",
    "    \n",
    "    # Compute global statistics for fallback\n",
    "    if fallback_method == 'global_std':\n",
    "        reasonable_stds = all_individual_stds[all_individual_stds > min_std_threshold]\n",
    "        global_std_fallback = np.median(reasonable_stds) if len(reasonable_stds) > 0 else min_std_threshold\n",
    "\n",
    "    # Second pass: apply normalization with determined threshold\n",
    "    normalization_values = np.zeros((neural_data.shape[0], 2))\n",
    "    small_std_count = 0\n",
    "    \n",
    "    for i in range(neural_data.shape[0]):\n",
    "        individual_mu, individual_std, individual_non_zero_neural_data = temp_normalization_data[i]\n",
    "        \n",
    "        # Handle very small standard deviations\n",
    "        if individual_std < min_std_threshold:\n",
    "            small_std_count += 1\n",
    "            if Verbose >= 1:\n",
    "                print(f'Neuron {i}: std too small ({individual_std:.6f}), using fallback method: {fallback_method}')\n",
    "            \n",
    "            if fallback_method == 'median_mad':\n",
    "                # Use Median Absolute Deviation (MAD) scaled to match standard deviation\n",
    "                mad = np.median(np.abs(individual_non_zero_neural_data - individual_mu))\n",
    "                individual_std = max(mad * 1.4826, min_std_threshold)  # 1.4826 scales MAD to match std for normal distribution\n",
    "            elif fallback_method == 'global_std':\n",
    "                individual_std = global_std_fallback\n",
    "            elif fallback_method == 'fixed_std':\n",
    "                individual_std = min_std_threshold\n",
    "            else:\n",
    "                individual_std = min_std_threshold\n",
    "        \n",
    "        normalization_values[i,0] = individual_mu\n",
    "        normalization_values[i,1] = individual_std\n",
    "        \n",
    "        if Verbose == 2:\n",
    "            if i % 50 == 0:\n",
    "                plt.hist(individual_non_zero_neural_data, bins=50)\n",
    "                plt.axvline(individual_mu, c='r', label=f'median = {(individual_mu)}')\n",
    "                plt.title(f'Individual Histogram Neuron #{i}')\n",
    "                plt.ylabel('Counts')\n",
    "                plt.xlabel('Activity Level ')\n",
    "                plt.legend()\n",
    "                plt.show()\n",
    "    \n",
    "    if Verbose >= 1 and small_std_count > 0:\n",
    "        print(f'Warning: {small_std_count}/{neural_data.shape[0]} neurons had std < {min_std_threshold:.6f}')\n",
    "        print(f'Used fallback method: {fallback_method}')\n",
    "        \n",
    "    # Show distribution of standard deviations for diagnostics\n",
    "    if Verbose >= 1:\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.hist(all_individual_stds, bins=50, alpha=0.7, label='Original STDs')\n",
    "        plt.axvline(min_std_threshold, color='red', linestyle='--', label=f'Threshold: {min_std_threshold:.6f}')\n",
    "        plt.xlabel('Standard Deviation')\n",
    "        plt.ylabel('Count')\n",
    "        plt.title('Distribution of Individual STDs')\n",
    "        plt.legend()\n",
    "        plt.yscale('log')\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        final_stds = normalization_values[:, 1]\n",
    "        plt.hist(final_stds, bins=50, alpha=0.7, label='Final STDs', color='green')\n",
    "        plt.axvline(min_std_threshold, color='red', linestyle='--', label=f'Threshold: {min_std_threshold:.6f}')\n",
    "        plt.xlabel('Standard Deviation')\n",
    "        plt.ylabel('Count')\n",
    "        plt.title('Distribution of Final STDs')\n",
    "        plt.legend()\n",
    "        plt.yscale('log')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # Apply normalization with the robust std values\n",
    "    normalized_data = (neural_data_original - normalization_values[:, [0]]) / normalization_values[:, [1]]\n",
    "    \n",
    "    return normalization_values, normalized_data\n",
    "all_neural_data = load_pickle('C_norm.pkl')\n",
    "cleaned_data = clean_neural_data(all_neural_data, good_exps, zero_threshold=0.5)\n",
    "use_first = False\n",
    "normalize = True\n",
    "counter = 0\n",
    "if use_first: \n",
    "    all_touch = load_pickle('touch_first_mini_frame.pkl')\n",
    "else: \n",
    "    all_touch = load_pickle('touch_mini_frame.pkl')\n",
    "for animal in cleaned_data:\n",
    "    for stimuli in cleaned_data[animal]:\n",
    "        \n",
    "        print(f'animal: {animal}')\n",
    "        print(f'stimuli: {stimuli}')\n",
    "        try: \n",
    "            neural_data = cleaned_data[animal][stimuli]\n",
    "        except KeyError as ke:\n",
    "            print(f'this animal {animal}, does not have neural data with this stimuli: {stimuli}')\n",
    "            continue\n",
    "\n",
    "        try: \n",
    "            touches = all_touch[animal][stimuli]\n",
    "        except KeyError as ke:\n",
    "            print(f'this animal {animal}, does have neural data but does not have touch sensor data for this stimuli: {stimuli}')\n",
    "            continue\n",
    "        if normalize:\n",
    "            verbose_level = 1 if counter == 0 else 0\n",
    "            _, neural_data_normalized = our_normalization(neural_data, Verbose=verbose_level, \n",
    "                                             min_std_threshold='auto', \n",
    "                                             fallback_method='median_mad')\n",
    "            counter +=1\n",
    "            fig, axes = plt.subplots(1, 2, figsize=(12, 8))\n",
    "            vis_data = neural_data_normalized\n",
    "            vis_data[vis_data < 0] = 0\n",
    "            vis_data = np.sqrt(vis_data)\n",
    "            # Plot both heatmaps and get the image objects\n",
    "            ax0, im0 = plot_heat_touch(vis_data, touches, \n",
    "                                     title='Normalized Neural Data', ax=axes[0])\n",
    "            ax1, im1 = plot_heat_touch(neural_data, touches, \n",
    "                                     title='Raw Neural Data', ax=axes[1])\n",
    "            \n",
    "            # Add colorbars for both subplots\n",
    "            plt.colorbar(im0, ax=axes[0], label='Normalized Activity')\n",
    "            plt.colorbar(im1, ax=axes[1], label='Raw Activity')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        # offset = np.nanmin(neural_data)\n",
    "        # neural_data = np.sqrt(neural_data - offset)\n",
    "        # neural_data = neural_data + np.sqrt(np.abs(offset))\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2227781e",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92168e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "for mouse, stimuli in cleaned_data.items():\n",
    "    print(mouse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080ac0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate along time dimensions (data will be T x N)\n",
    "normalized_data = {}\n",
    "for mouse_name, mouse in cleaned_data.items():\n",
    "    normalized_data[mouse_name] = {}\n",
    "    for stim_name, responses in mouse.items():\n",
    "        _, normalized_data[mouse_name][stim_name] = our_normalization(responses, Verbose=verbose_level, \n",
    "                                             min_std_threshold='auto', \n",
    "                                             fallback_method='median_mad')\n",
    "data = [x for x in normalized_data['3012'].values()]\n",
    "data = np.hstack(data)\n",
    "data_vis = data\n",
    "data_vis[data_vis < 0] = 0\n",
    "data_vis = np.sqrt(data_vis)\n",
    "plt.imshow(data_vis, extent=[0, data.shape[1]/HZ/25, 0, data.shape[0]])\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "data2 = [x for x in cleaned_data['3012'].values()]\n",
    "data2 = np.hstack(data2)\n",
    "plt.imshow(data2, extent=[0, data2.shape[1]/HZ/25, 0, data2.shape[0]])\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5d4bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "model = NMF(n_components=2, init='random', random_state=0)\n",
    "W = model.fit_transform(X)\n",
    "H = model.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06921510",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "data_centered = data - np.mean(data, axis=1, keepdims=True)\n",
    "# plt.imshow(data_centered, extent=[0, data_centered.shape[1]/HZ/25, 0, data_centered.shape[0]])\n",
    "# plt.colorbar()\n",
    "\n",
    "data_cov = (data_centered @ data_centered.T) / data_centered.shape[1]\n",
    "# plt.imshow(data_cov)\n",
    "# plt.colorbar()\n",
    "vals, vectors = np.linalg.eig(data_cov)\n",
    "# plt.bar(np.arange(0,vals.shape[0]), vals)\n",
    "print(data_centered.shape)\n",
    "print(vectors.shape)\n",
    "for i in range(1):\n",
    "    plt.plot(data_centered.T @ vectors[:, i])\n",
    "\n",
    "for i in range(10):\n",
    "    vectors_sorted = np.argsort(vectors[:, i])\n",
    "    plt.imshow(data_vis[vectors_sorted, :], extent=[0, data_centered.shape[1]/HZ/25, 0, data_centered.shape[0]])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620ca0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "model = NMF(n_components=10, init='random', random_state=0)\n",
    "W = model.fit_transform(data.T)\n",
    "H = model.components_\n",
    "print(H.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea7d99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(W)\n",
    "plt.show()\n",
    "for i in range(10):\n",
    "    vectors_sorted = np.argsort(H[i, :])\n",
    "    plt.imshow(data_vis[vectors_sorted[H[i, vectors_sorted] > np.mean(H[i,:])], :], aspect='auto')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da824abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_neural_data = load_pickle('C_norm.pkl')\n",
    "all_exps = {}\n",
    "\n",
    "for animal in good_exps:\n",
    "    for stimuli in good_exps[animal]:\n",
    "        # Get the data for this experiment\n",
    "        data = all_neural_data[animal][stimuli]\n",
    "        \n",
    "        # Calculate zero proportion for each neuron\n",
    "        zero_proportions = []\n",
    "        for neuron_number in range(data.shape[0]):\n",
    "            # Count zeros and calculate proportion\n",
    "            zero_count = np.sum(data[neuron_number, :] == 0)\n",
    "            zero_proportion = zero_count / data.shape[1]\n",
    "            zero_proportions.append(zero_proportion)\n",
    "        \n",
    "        # Store the results using tuple as key\n",
    "        all_exps[(animal, stimuli)] = zero_proportions\n",
    "all_exps\n",
    "for animal, stimuli in list(all_exps.keys()):\n",
    "    plt.hist(all_exps[(animal, stimuli)], bins=50)\n",
    "    plt.xlabel('Zero Proportion')\n",
    "    plt.ylabel('Number of Neurons')\n",
    "    plt.title(f'Distribution of Zero Proportions - {animal}, {stimuli}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a042c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(all_exps.keys()))\n",
    "for animal, stimuli in list(all_exps.keys()):\n",
    "    print(animal, stimuli )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ba8bf7",
   "metadata": {},
   "source": [
    "# Working with individual stuff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6dea51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hist_normalizer(neural_data):\n",
    "    pop_sum = np.sum(neural_data, axis=0)\n",
    "    pop_med = np.median(pop_sum)\n",
    "\n",
    "    pop_le_med_std_theres = pop_sum[pop_sum < pop_med].std()\n",
    "\n",
    "    pop_sum_le_med = pop_sum[pop_sum < pop_med] \n",
    "    pop_le_med_std_ours = np.sqrt(np.sum((pop_sum_le_med - pop_med)**2)/(len(pop_sum_le_med)-1))\n",
    "\n",
    "    pop_le_med_2med_ours = pop_med + (2 * pop_le_med_std_ours)\n",
    "    pop_le_med_2med_theres = pop_med + (2 * pop_le_med_std_theres)\n",
    "\n",
    "\n",
    "    plt.hist(pop_sum, bins=50)\n",
    "    plt.axvline(pop_med, c='r', label=f'median = {round(pop_med)}')\n",
    "    plt.axvline(pop_le_med_2med_ours, c='g', label=f'2$\\sigma$ (our method) + median = {round(pop_le_med_2med_ours)}')\n",
    "    plt.axvline(pop_le_med_2med_theres, c='k', label=f'2$\\sigma$ + median = {round(pop_le_med_2med_theres)}')\n",
    "\n",
    "    plt.title('Population Sum Histogram')\n",
    "    plt.ylabel('Counts')\n",
    "    plt.xlabel('Activity Level ')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f30c23",
   "metadata": {},
   "source": [
    "So sometimes we have the neural data but not the tapping data? Yes!\n",
    "\n",
    "If a glomeruli is present in animal A with stimulus presentation B, is that same glomeruli present (and indexed the same) throughout each experiment? "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vnoaob",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
